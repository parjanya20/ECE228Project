{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "#For reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from data import create_synth_data as csd\n",
    "from scripts import bbse, methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,g = csd.create_synthetic_data(10000)\n",
    "X_train, X_test = X[g!=3], X[g==3]\n",
    "y_train, y_test = y[g!=3], y[g==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping!\n",
      "Early stopping!\n",
      "Early stopping!\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_ours_prob, y_pred_val_ours_prob, y_val_ours, y1 = methods.ours(X_train, y_train, g[g!=3], X_test,1.0)\n",
    "y_pred_test_ours = np.argmax(y_pred_test_ours_prob, axis=1)\n",
    "y_pred_val_ours = np.argmax(y_pred_val_ours_prob, axis=1)\n",
    "y_pred_test_erm_prob , y_pred_val_erm_prob, y_val_erm= methods.erm(X_train, y_train, X_test)\n",
    "y_pred_test_erm = np.argmax(y_pred_test_erm_prob, axis=1)\n",
    "y_pred_val_erm = np.argmax(y_pred_val_erm_prob, axis=1)\n",
    "\n",
    "y_pred_test_irm_prob = methods.irm(X_train, y_train, g[g!=3], X_test,0.1)\n",
    "y_pred_test_irm = np.argmax(y_pred_test_irm_prob, axis=1)\n",
    "y_pred_test_dro_prob = methods.group_dro(X_train, y_train, g[g!=3], X_test)\n",
    "y_pred_test_dro = np.argmax(y_pred_test_dro_prob, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748,)\n",
      "(748,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_val_erm.shape)\n",
    "print(y_val_erm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy Ours:  0.9231074118113357\n",
      "Test accuracy ERM:  0.8751486325802615\n",
      "Test accuracy IRM:  0.8620689655172413\n",
      "Test accuracy DRO:  0.896551724137931\n",
      "Confusion matrix for ERM on validation data: \n",
      "0.049\n",
      "0.912\n",
      "Confusion matrix for Ours on validation data: \n",
      "0.065\n",
      "0.919\n",
      "Confusion matrix for ERM on test data: \n",
      "0.052\n",
      "0.857\n",
      "Confusion matrix for Ours on test data: \n",
      "0.074\n",
      "0.922\n"
     ]
    }
   ],
   "source": [
    "#Print test accuracy for ERM, IRM and DRO and ours\n",
    "print(\"Test accuracy Ours: \", np.mean(y_pred_test_ours == y_test))\n",
    "print(\"Test accuracy ERM: \", np.mean(y_pred_test_erm == y_test))\n",
    "print(\"Test accuracy IRM: \", np.mean(y_pred_test_irm == y_test))\n",
    "print(\"Test accuracy DRO: \", np.mean(y_pred_test_dro == y_test))\n",
    "\n",
    "\n",
    "#Print normalised confusion matrix for ERM and ours for validation and test data separately i.e. the entries should represent p(y_hat|y) and each row should sum to 1\n",
    "print(\"Confusion matrix for ERM on validation data: \")\n",
    "print(np.around(np.mean(y_pred_val_erm[y_val_erm==0], axis=0), 3))\n",
    "print(np.around(np.mean(y_pred_val_erm[y_val_erm==1], axis=0), 3))\n",
    "print(\"Confusion matrix for Ours on validation data: \")\n",
    "print(np.around(np.mean(y_pred_val_ours[y_val_ours==0], axis=0), 3))\n",
    "print(np.around(np.mean(y_pred_val_ours[y_val_ours==1], axis=0), 3))\n",
    "print(\"Confusion matrix for ERM on test data: \")\n",
    "print(np.around(np.mean(y_pred_test_erm[y_test==0], axis=0), 3))\n",
    "print(np.around(np.mean(y_pred_test_erm[y_test==1], axis=0), 3))\n",
    "print(\"Confusion matrix for Ours on test data: \")\n",
    "print(np.around(np.mean(y_pred_test_ours[y_test==0], axis=0), 3)) \n",
    "print(np.around(np.mean(y_pred_test_ours[y_test==1], axis=0), 3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24737344 0.75262656] [0.62165975 0.37834425]\n",
      "Test accuracy ERM BBSE:  0.926674593737614\n"
     ]
    }
   ],
   "source": [
    "qy_hat = np.zeros(2)\n",
    "qy_hat[0] = np.mean(y_pred_test_erm== 0)\n",
    "qy_hat[1] = np.mean(y_pred_test_erm == 1)\n",
    "\n",
    "qy, py = bbse.estimate_test_py(y_val_erm,y_pred_val_erm,qy_hat)\n",
    "print(qy, py)\n",
    "y_pred_test_erm_prob_bbse = bbse.recalibrate(y_pred_test_erm_prob, qy, py)\n",
    "#Calculate accuracy of recalibrated ERM\n",
    "y_pred_test_erm_bbse = np.argmax(y_pred_test_erm_prob_bbse, axis=1)\n",
    "print(\"Test accuracy ERM BBSE: \", np.mean(y_pred_test_erm_bbse == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19160753 0.80839247] [0.62165975 0.37834425]\n",
      "Test accuracy Ours BBSE:  0.9397542608006342\n"
     ]
    }
   ],
   "source": [
    "#Do BBSE for ours instead of ERM\n",
    "qy_hat = np.zeros(2)\n",
    "qy_hat[0] = np.mean(y_pred_test_ours== 0)\n",
    "qy_hat[1] = np.mean(y_pred_test_ours == 1)\n",
    "\n",
    "qy, py = bbse.estimate_test_py(y_val_ours,y_pred_val_ours,qy_hat)\n",
    "print(qy, py)\n",
    "y_pred_test_ours_prob_bbse = bbse.recalibrate(y_pred_test_ours_prob, qy, py)\n",
    "#Calculate accuracy of recalibrated ours\n",
    "y_pred_test_ours_bbse = np.argmax(y_pred_test_ours_prob_bbse, axis=1)\n",
    "print(\"Test accuracy Ours BBSE: \", np.mean(y_pred_test_ours_bbse == y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
